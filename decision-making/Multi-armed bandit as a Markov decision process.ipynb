{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "",
  "signature": "sha256:849fc92aab12e5754bda76b85fa026f523c39e1b0dd7401f10dae965408fd278"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Multi-armed bandit as a Markov decision process\n",
      "\n",
      "Let's model the Bernouilli multi-armed bandit. The Bernoulli MBA is an $N$-armed bandit where each arm gives binary rewards according to some probability:\n",
      "\n",
      "$r_i \\sim Bernouilli(\\mu_i)$ \n",
      "\n",
      "Here $i$ is the index of the arm. Let's model this as a Markov decision process. The state is going to be defined as:\n",
      "\n",
      "$s(t) = (\\alpha_1, \\beta_1, \\ldots, \\alpha_N, \\beta_N, r_t)$\n",
      "\n",
      "$\\alpha_i$ is the number of successes encountered so far when pulling arm $i$. $\\beta_i$ is, similarly, the number of failures encountered when pulling that arm. $r_t$ is the reward, either 0 or 1, from the last trial. \n",
      "\n",
      "Assuming a uniform prior on $\\mu_i$, the posterior distribution of the $\\mu_i$ in a given state are:\n",
      "\n",
      "$p(\\mu_i|s(t)) = Beta(\\alpha_i+1,\\beta_i+1)$\n",
      "\n",
      "When we're in a given state, we have the choice of performing one of $N$ actions, corresponding to pulling each of the arms. Let's call pulling the $i$'th arm $a_i$. This will put us in a new state, with a certain probability. The new state will be same for arms not equal to i. For the $i$'th arm, we have:\n",
      "\n",
      "$s(t+1) = (\\ldots \\alpha_i + 1, \\beta_i \\ldots 1)$ with probability $(\\alpha_i+1)/(\\alpha_i+\\beta_i+2)$\n",
      "\n",
      "$s(t+1) = (\\ldots \\alpha_i, \\beta_i + 1 \\ldots 0)$ with probability $(\\beta_i+1)/(\\alpha_i+\\beta_i+2)$\n",
      "\n",
      "We can solve exactly for this MDP, e.g. using value iteration, given that it's small enough. For $M$ trials, the state space is $M^{2N}$ - it's possible to solve the 2-armed bandit for 10-20 trials this way, but it grows exponentially fast."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import itertools\n",
      "import numpy as np\n",
      "from pprint import pprint\n",
      "\n",
      "def sorted_values(dict_):\n",
      "    return [dict_[x] for x in sorted(dict_)]\n",
      "\n",
      "def solve_bmab_value_iteration(N_arms, M_trials, gamma=1,\n",
      "                               max_iter=10, conv_crit = .01):\n",
      "    util = {}\n",
      "    \n",
      "    # Initialize every state to utility 0.\n",
      "    state_ranges = [range(M_trials+1) for x in range(N_arms*2)]\n",
      "    # The reward state\n",
      "    state_ranges.append(range(2))\n",
      "    for state in itertools.product(*state_ranges):\n",
      "        # Some states are impossible to reach.\n",
      "        if sum(state[:-1]) > M_trials:\n",
      "            # A state with the total of alphas and betas greater than \n",
      "            # the number of trials.\n",
      "            continue\n",
      "            \n",
      "        if sum(state[:-1:2]) == 0 and state[-1] == 1:\n",
      "            # A state with a reward but alphas all equal to 0.\n",
      "            continue\n",
      "            \n",
      "        if sum(state[:-1:2]) == M_trials and state[-1] == 0:\n",
      "            # A state with no reward but alphas adding up to M_trials.\n",
      "            continue\n",
      "            \n",
      "        util[state] = 0\n",
      "    \n",
      "    # Main loop.\n",
      "    converged = False\n",
      "    new_util = util.copy()\n",
      "    opt_actions = {}\n",
      "    for j in range(max_iter):\n",
      "        # Line 5 of value iteration\n",
      "        for state in util.keys():\n",
      "            reward = state[-1]\n",
      "            \n",
      "            # Terminal state.\n",
      "            if sum(state[:-1]) == M_trials:\n",
      "                new_util[state] = reward\n",
      "                continue\n",
      "            \n",
      "            values = np.zeros(N_arms)\n",
      "            \n",
      "            # Consider every action\n",
      "            for i in range(N_arms):\n",
      "                # Successes and failure for this state.\n",
      "                alpha = state[i*2]\n",
      "                beta  = state[i*2+1]\n",
      "                \n",
      "                # Two possible outcomes: either that arm gets rewarded,\n",
      "                # or not.\n",
      "                # Transition to unrewarded state:\n",
      "                state0 = list(state)\n",
      "                state0[-1] = 0\n",
      "                state0[2*i+1] += 1\n",
      "                state0 = tuple(state0)\n",
      "                \n",
      "                # The probability that we'll transition to this unrewarded state.\n",
      "                p_state0 = (beta + 1) / float(alpha + beta + 2)\n",
      "                \n",
      "                # Rewarded state.\n",
      "                state1 = list(state)\n",
      "                state1[-1] = 1\n",
      "                state1[2*i] += 1\n",
      "                state1 = tuple(state1)\n",
      "                \n",
      "                p_state1 = 1 - p_state0\n",
      "                try:\n",
      "                    value = gamma*(util[state0]*p_state0 + \n",
      "                                   util[state1]*p_state1)\n",
      "                except KeyError,e:\n",
      "                    print state\n",
      "                    print state0\n",
      "                    print state1\n",
      "                    raise e\n",
      "                    \n",
      "                #print state0, util[state0], p_state0\n",
      "                #print state1, util[state1], p_state1\n",
      "                values[i] = value\n",
      "                \n",
      "            #print state, values, reward\n",
      "            new_util[state] = reward + np.max(values)\n",
      "            opt_actions[state] = np.argmax(values)\n",
      "            \n",
      "        # Consider the difference between the new util\n",
      "        # and the old util.\n",
      "        max_diff = np.max(abs(np.array(sorted_values(util)) - np.array(sorted_values(new_util))))\n",
      "        util = new_util.copy()\n",
      "        \n",
      "        print \"Iteration %d, max diff = %.5f\" % (j, max_diff)\n",
      "        if max_diff < conv_crit:\n",
      "            converged = True\n",
      "            break\n",
      "            \n",
      "        #pprint(util)\n",
      "            \n",
      "    if converged:\n",
      "        print \"Converged after %d iterations\" % j\n",
      "    else:\n",
      "        print \"Not converged after %d iterations\" % max_iter\n",
      "        \n",
      "    return util, opt_actions\n",
      "\n",
      "util, opt_actions = solve_bmab_value_iteration(2, 2, max_iter=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration 0, max diff = 1.00000\n",
        "Iteration 1, max diff = 0.66667\n",
        "Iteration 2, max diff = 0.58333\n",
        "Iteration 3, max diff = 0.00000\n",
        "Converged after 3 iterations\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "opt_actions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "{(0, 0, 0, 0, 0): 0,\n",
        " (0, 0, 0, 1, 0): 0,\n",
        " (0, 0, 1, 0, 0): 1,\n",
        " (0, 0, 1, 0, 1): 1,\n",
        " (0, 1, 0, 0, 0): 1,\n",
        " (1, 0, 0, 0, 0): 0,\n",
        " (1, 0, 0, 0, 1): 0}"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the 2-armed, 2-trial Bernoulli bandit, the strategy is simple: pick the first arm. If it rewards, then pick it again. If not, pick the other. Note that this is the same as most sensible strategies, for instance greedy or UCB. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "util"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "{(0, 0, 0, 0, 0): 1.0833333333333335,\n",
        " (0, 0, 0, 1, 0): 0.5,\n",
        " (0, 0, 0, 2, 0): 0,\n",
        " (0, 0, 1, 0, 0): 0.66666666666666674,\n",
        " (0, 0, 1, 0, 1): 1.6666666666666667,\n",
        " (0, 0, 1, 1, 0): 0,\n",
        " (0, 0, 1, 1, 1): 1,\n",
        " (0, 0, 2, 0, 1): 1,\n",
        " (0, 1, 0, 0, 0): 0.5,\n",
        " (0, 1, 0, 1, 0): 0,\n",
        " (0, 1, 1, 0, 0): 0,\n",
        " (0, 1, 1, 0, 1): 1,\n",
        " (0, 2, 0, 0, 0): 0,\n",
        " (1, 0, 0, 0, 0): 0.66666666666666674,\n",
        " (1, 0, 0, 0, 1): 1.6666666666666667,\n",
        " (1, 0, 0, 1, 0): 0,\n",
        " (1, 0, 0, 1, 1): 1,\n",
        " (1, 0, 1, 0, 1): 1,\n",
        " (1, 1, 0, 0, 0): 0,\n",
        " (1, 1, 0, 0, 1): 1,\n",
        " (2, 0, 0, 0, 1): 1}"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the utility of the root node is 1.08 - what does that mean? If we get rewarded in the initial trial, that means that the posterior for the mean of that arm is .67. OTOH, when we fail on the first trial, we can still pick the other arm, which still has a posterior mean of .5. Thus, we have rewards:\n",
      "\n",
      "  * +2 with probability .5*2/3\n",
      "  * +1 with prob        .5*1/3\n",
      "  * +1 with prob        .5*.5\n",
      "  * +0 with prob .5*.5\n",
      "  \n",
      "That means the expected total reward is: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "2*.5*2.0/3.0 + .5/3.0 + .5*.5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "1.0833333333333333"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And that's what utility means in this context.\n",
      "Let's see about the 3-trial 2-armed bandit:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "util, opt_actions = solve_bmab_value_iteration(2, 3, max_iter=5)\n",
      "opt_actions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration 0, max diff = 1.00000\n",
        "Iteration 1, max diff = 0.75000\n",
        "Iteration 2, max diff = 0.66667\n",
        "Iteration 3, max diff = 0.58333\n",
        "Iteration 4, max diff = 0.00000\n",
        "Converged after 4 iterations\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "{(0, 0, 0, 0, 0): 0,\n",
        " (0, 0, 0, 1, 0): 0,\n",
        " (0, 0, 0, 2, 0): 0,\n",
        " (0, 0, 1, 0, 0): 1,\n",
        " (0, 0, 1, 0, 1): 1,\n",
        " (0, 0, 1, 1, 0): 0,\n",
        " (0, 0, 1, 1, 1): 0,\n",
        " (0, 0, 2, 0, 0): 1,\n",
        " (0, 0, 2, 0, 1): 1,\n",
        " (0, 1, 0, 0, 0): 1,\n",
        " (0, 1, 0, 1, 0): 0,\n",
        " (0, 1, 1, 0, 0): 1,\n",
        " (0, 1, 1, 0, 1): 1,\n",
        " (0, 2, 0, 0, 0): 1,\n",
        " (1, 0, 0, 0, 0): 0,\n",
        " (1, 0, 0, 0, 1): 0,\n",
        " (1, 0, 0, 1, 0): 0,\n",
        " (1, 0, 0, 1, 1): 0,\n",
        " (1, 0, 1, 0, 0): 0,\n",
        " (1, 0, 1, 0, 1): 0,\n",
        " (1, 1, 0, 0, 0): 0,\n",
        " (1, 1, 0, 0, 1): 0,\n",
        " (2, 0, 0, 0, 0): 0,\n",
        " (2, 0, 0, 0, 1): 0}"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The optimal strategy goes: pick arm 0. If it rewards, pick it again for the next 2 trials.\n",
      "If it doesn't reward, then pick arm 1. If that rewards, keep that one. If it doesn't, pick 0 again.\n",
      "\n",
      "Let's see with 4:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "util, opt_actions = solve_bmab_value_iteration(2, 4, max_iter=6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration 0, max diff = 1.00000\n",
        "Iteration 1, max diff = 0.80000\n",
        "Iteration 2, max diff = 0.75000\n",
        "Iteration 3, max diff = 0.69444\n",
        "Iteration 4, max diff = 0.61111\n",
        "Iteration 5, max diff = 0.00000\n",
        "Converged after 5 iterations\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "{(0, 0, 0, 0, 0): 0,\n",
        " (0, 0, 0, 1, 0): 0,\n",
        " (0, 0, 0, 2, 0): 0,\n",
        " (0, 0, 0, 3, 0): 0,\n",
        " (0, 0, 1, 0, 0): 1,\n",
        " (0, 0, 1, 0, 1): 1,\n",
        " (0, 0, 1, 1, 0): 0,\n",
        " (0, 0, 1, 1, 1): 0,\n",
        " (0, 0, 1, 2, 0): 0,\n",
        " (0, 0, 1, 2, 1): 0,\n",
        " (0, 0, 2, 0, 0): 1,\n",
        " (0, 0, 2, 0, 1): 1,\n",
        " (0, 0, 2, 1, 0): 1,\n",
        " (0, 0, 2, 1, 1): 1,\n",
        " (0, 0, 3, 0, 0): 1,\n",
        " (0, 0, 3, 0, 1): 1,\n",
        " (0, 1, 0, 0, 0): 1,\n",
        " (0, 1, 0, 1, 0): 0,\n",
        " (0, 1, 0, 2, 0): 0,\n",
        " (0, 1, 1, 0, 0): 1,\n",
        " (0, 1, 1, 0, 1): 1,\n",
        " (0, 1, 1, 1, 0): 1,\n",
        " (0, 1, 1, 1, 1): 1,\n",
        " (0, 1, 2, 0, 0): 1,\n",
        " (0, 1, 2, 0, 1): 1,\n",
        " (0, 2, 0, 0, 0): 1,\n",
        " (0, 2, 0, 1, 0): 1,\n",
        " (0, 2, 1, 0, 0): 1,\n",
        " (0, 2, 1, 0, 1): 1,\n",
        " (0, 3, 0, 0, 0): 1,\n",
        " (1, 0, 0, 0, 0): 0,\n",
        " (1, 0, 0, 0, 1): 0,\n",
        " (1, 0, 0, 1, 0): 0,\n",
        " (1, 0, 0, 1, 1): 0,\n",
        " (1, 0, 0, 2, 0): 0,\n",
        " (1, 0, 0, 2, 1): 0,\n",
        " (1, 0, 1, 0, 0): 0,\n",
        " (1, 0, 1, 0, 1): 0,\n",
        " (1, 0, 1, 1, 0): 0,\n",
        " (1, 0, 1, 1, 1): 0,\n",
        " (1, 0, 2, 0, 0): 1,\n",
        " (1, 0, 2, 0, 1): 1,\n",
        " (1, 1, 0, 0, 0): 1,\n",
        " (1, 1, 0, 0, 1): 1,\n",
        " (1, 1, 0, 1, 0): 0,\n",
        " (1, 1, 0, 1, 1): 0,\n",
        " (1, 1, 1, 0, 0): 1,\n",
        " (1, 1, 1, 0, 1): 1,\n",
        " (1, 2, 0, 0, 0): 1,\n",
        " (1, 2, 0, 0, 1): 1,\n",
        " (2, 0, 0, 0, 0): 0,\n",
        " (2, 0, 0, 0, 1): 0,\n",
        " (2, 0, 0, 1, 0): 0,\n",
        " (2, 0, 0, 1, 1): 0,\n",
        " (2, 0, 1, 0, 0): 0,\n",
        " (2, 0, 1, 0, 1): 0,\n",
        " (2, 1, 0, 0, 0): 0,\n",
        " (2, 1, 0, 0, 1): 0,\n",
        " (3, 0, 0, 0, 0): 0,\n",
        " (3, 0, 0, 0, 1): 0}"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What's interesting here is that value iteration always converges in M_trials + 1 iterations - information only travels backwards through time - much as in Viterbi in the context of HMMs. If we're only interested in the next best action given the current state, it might be possible to iterate backwards through time, starting from the terminal states, throwing away the latest data as we go along. We'll try this next, as well as a few other things:\n",
      "\n",
      "  - Approximate the optimal strategy by training a supervised network - how does this pseudo-optimal strategy compare with other deterministic strategies, like UCB?\n",
      "  - How good is a strategy that is myopic, that is, only looks at a few steps in advance?\n",
      "  - What does policy iteration look like in this context?\n",
      "  - How does this all compare to MCTS?\n",
      "  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "util"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "{(0, 0, 0, 0, 0): 1,\n",
        " (0, 0, 0, 1, 0): 1,\n",
        " (0, 0, 0, 2, 0): 0,\n",
        " (0, 0, 1, 0, 0): 1,\n",
        " (0, 0, 1, 0, 1): 2,\n",
        " (0, 0, 1, 1, 0): 0,\n",
        " (0, 0, 1, 1, 1): 1,\n",
        " (0, 0, 2, 0, 0): 0,\n",
        " (0, 0, 2, 0, 1): 1,\n",
        " (0, 1, 0, 0, 0): 0,\n",
        " (0, 1, 0, 1, 0): 0,\n",
        " (0, 1, 1, 0, 0): 0,\n",
        " (0, 1, 1, 0, 1): 1,\n",
        " (0, 2, 0, 0, 0): 0,\n",
        " (1, 0, 0, 0, 0): 0,\n",
        " (1, 0, 0, 0, 1): 1,\n",
        " (1, 0, 0, 1, 0): 0,\n",
        " (1, 0, 0, 1, 1): 1,\n",
        " (1, 0, 1, 0, 0): 0,\n",
        " (1, 0, 1, 0, 1): 1,\n",
        " (1, 1, 0, 0, 0): 0,\n",
        " (1, 1, 0, 0, 1): 1,\n",
        " (2, 0, 0, 0, 0): 0,\n",
        " (2, 0, 0, 0, 1): 1}"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pprint\n",
      "\n",
      "pprint.pprint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    }
   ],
   "metadata": {}
  }
 ]
}